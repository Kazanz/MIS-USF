# Week 1 Day 2

Go find out what a Sampling Distribution is.

You can ask any questions at the beginning of class.  Related to statistics or not.

Sampling distribution is a probabitliy distribution.
prob dist: ares something that provide probabilities associated with some outcome variable form a population standpoint.

If you can read one print out you can read them all.  Its all the same stats just in a different order.  R, SAS, etc.

How does the f value correspand with sifnificant factor P.
If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time. A large F ratio means that the variation among group means is more than you would expect to see by chance.


Chapter 3: SLR (Simple linear regression)

Estimated the Beta Coefficient (look that up)
Standard deviation of the error term (how accurate the term might be)
The coeff var (look that up as well)

Step 5:
Statitically evaluate utility of the model
Confidence Interval andd/or hypothesis test

Ok lets conduct a test.  We will test if beta 1 the slop is eqaul to 0 (we are not saying the equation that B1 is not a good indicator, just that this model is not useful).
Alt ahypo is that Beta coeffcient does not equal zero. (it could be useful, but may not be great)
test statistic: t = estimaed B1/std. error of est.
Sampling Distrib of B1 is Normal when theta known

Normal prob distribution- A regular bell curve.
Ha all kind of different forms

There is avariable being measured in each sampling distribution.
Sigma is a standard deviation.

You create a sampling distribution.  We take n (is fixed lets say 15) colelct x and y 15 observations for the first sample.  When you think about the samling distirbution lets do it over and over again. (Another sample of 15 [n])
Now put all these estimated slopes in the Box.  Do it again, over and over. Now how are all these slopes distributed.  That is the sampling distirbution.  If you sample enough, you will eventually have a normal distribution.
sample means (slope estimates of beta) is what we get, and that comes from x, y in the sample data.

Is the estimate fo the beta1 far enough away from zero

Type 1 Error where you end up rejecting the null hypothesis when it is true. alpha is probablity of Type 1 error.
Type 2 Error where you accecpt the null hypo when it is not true.  Beta is probability of Type 2 error.
Alpha errors can be controlled, but beta errors cannot.

.01 .05 .1 is the highest you want for the alpha value.

Pick a small alpha and be concerned about rejecting a null hypothesis.

**What the the parameter estimate?** (y intercept right?
T is the test statistic value (get it by divideing parameter estimate divided by the standard error)
Standard error is the standard deviation of the sampling distribution you are focusing in on.  (see above for beta1)

If the T value is small the P is going to be big and vice versa.

P-values  P (probability).  Assuming that null hypothesis is true
SAS will automatically do a two-tail test by default.  If you want one tail just take half.
as long as the test statistica value agrees with the sign in Ha we can just half it.  Otherwise you have to take the opposite (multiply by *-1)
P(t > 12.53)

You pick an alpha value to compare to the p value.
If alpha > p_value reject the null hypothesis.

Neamon and Perason come up with the concept of hypothesis testing.  (They called it a rejection regino instead of p values)

The most abused statistic in research is P-Values.  Everybody wants lots of p values.  If you make N big enough any P-Value can look good in terms of rejecting the null hypothesis.
Usually you are better off making a judgement on the confidence interval.

The higher N-Value the confidence interval will shrink.
If we look at the confidence interval and the max and min is higher thatn 0 then we can reject the null hypothesis
P is the smallest alpha you could use and still reject the null hypothesis.
You really should decide on the alpha value ahead of time.

P value has nothing to do with predicting.  It is only for suggesting the probability of error when rejecting the null hypothesis (a zero slope) for one of the vairables in relation to another.
It is not 95% of the time we are saying it is 95% confident.  We know there is a fixed slope so we cannot say 95% of the time, because the slope is fixed.
In other words, every time you take a n sample. 95% of the time the slope will be in between these numbers.
Lower the confidence interval and we get a smalled range.

Numerical descriptive mesaures of model adequacy
Coefficient of correlation, r
Coffecient of determination, R^2
In SLR, r = +- sqrt(R^2) if the estimate of the beta-hat is negative rsquared will be negative etc.
Terms (not really important) y-bar y-hat
How much better can you do using a model with X in it compared to the mean of Ys.
R^2 is going to tell us how much better we do than a staight line model without X.
Not going to be able to give something a "golden" R^2 value.

If you add a bunch of different explanatory variable, or X variables.
Degrees of Freedom total are always going to be the sum of Xs  If n-1 = # of Xs, R^2 can be forced to 1
r measures the strnegth of the linear relationship between y and x
r = .96 -> evidence of strong positive linear relationship.  The closer to |1| the stronger linear relationship.  The closer to zero the weaker linear relationship. (In other words you cannot say it is not a good predictor)
