PRESS: Predicted Residual Error Sum of Squares
v = yi-yhat(i) 
v *= v
SUM(v)


# Variable Screening Methods:
*Chapter 6 in the book*

Edxecutive compensation data
7QNs, 3QLs
Complete second order model

Stage 1 (only quantitative): 7 (first order terms + 21 (interactions every combinations of those 7) + 7 (second order terms) = 37 terms
Stage 2 (only (qualitative): 3(each qualitative) + 3(two way interactions) + 1 (3 way interaction) = 7 terms
Stage 3 (every term in stage 1 interacted with stage 2): 35*7 = 245 terms

Ok what subset of these 10 models would be the best to statr with.

Need to reduce the # IVs in the model building
Select the "Most" important x s
Eliminate "least" important x s

## Stepwise Regression

Doesnt care about quantitative or qualitative.
Only apply to main effect first order terms.
Main effect term is the original term in the first order.

Step 1: Fit all possible 1-var models. and select the best variable by conducting a T-test on each model
Step 2: Git all possible 2-var models where the first var is the model from the last set.  Pick the best again. 
Step 3: etc...
Continue until no more significant x s.

Ho: Bi = 0
P-value associated with T value. The default alpha level in SAS is .15

Way too many t tests being performed, so you will almost definately get a Type I error.

If you treat it as a model building approach.  You will get too many errors.
Stepwise Regressoin is a way to get ready for the model building process.

Backwords regression takes out the least significant variables.

### Caveots
- Large # of t-tests performed, inglates P (at least 1 Type 1 Error)
- No higher-order terms in  final stepwise model (curvature, interaction)
- Should we indlude these terms in list
- SAS default is to use alpha .15 for each test
- How to handle QL variables.  If you make dummy variables add a 1 or 0 for each category.


## All-Possible Regressions

Choose model stat: R^2, R^2-adj, s, Cp, PRESS
Step 1: Fit all possible 1-var models, and based on the statistic you picked, you pick the best selector
Step 2: Fit all possible 2-var models, and based on the statistic you picked, but does not maintain the variable from the first step.
etc.
Continue until fit model with all variables
Select "Best" model based on selected stat.

This is subjective, because for example if you pick r squared, the model with the most variables will always have a larger r squared.

Cp = You want the Cp to be low.
A model is unbiased with Cp is around n + 1

If you want to test gender bias or have variables you know you want in the model.
Remove them from the Variable Screening process then add them to the model after.

**For the final exam you all must find a research paper in your field of study and critique their linear regression.**


# Regression Pitfalls

- Observational Data: Difficult to establish "causality"
- Extrapolation: Prediction y for x-values outside range of sample data (watch for "hidden" extrapolation)
    - You want to plot the experimental region of your x values. Because when you pair up the two x s  even though individually they are in the proper range, together they may not be.
- Parameter estimates: Can all betas be estimated? 
    - n > # betas & df(Error) > 0
- Multicollinearity: x s correlated.
