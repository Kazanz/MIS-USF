# Overview of Project

Describe all the data
Whats your objective
Build your model
Residual analysis

# Chapter 8

## Residual Analysis

Applications: 1. aid in model selection
              2. check assumptions on e 

True Error: e = y - E(y)
Residual: e-hat = y - y-hat = observed - predicted
properties: if you add residuals up it will equal 0
            the standard deviation of the residual is the standard deviation of the model

"Expected Value" means "Mean"
"Standard error of the estimate" means "standard deviation".  I think?

### Assumption #1: E(e) = 0
Viloted if misspecified model
If true model is E(y) = Bo + B1x + b2x^2 and fit y = Bo + B1x + e can show E(e) = B2x^2 != 0

Detecting Lack of Fit - plot e-hat vs each x
Look for trends, patterns
no pattern usually means no problem

Plot Residuals vs the independent variable

Aways remember to do this residual plot for every dependent variable in the model.

Partial residuals 
You could do the partial residuals ONLY.  Since they show you if there is curvature and the nature of that curvature for the variable.


### Assumption #2:
Var(e) = std-dev^2 is constant
Equal and unequal variances

Tyically this is violeted when there is a relationship between the mean of Y and the variance of Y
This happens a lot in "count" or poisson data.  Number of Xs.

x -> E(y)
Var(e) = Var(y)

Plot residuals vs the predicted value of Y
(Nothing to do with the Xs)

There are no assumptions made on the Xs

### If found:  Variance-stabliizing transformation
Poisson data looks like a bullet: y* = sqrt(y)
Football type graph for binomial data: y* = sin^-1 * sqrt(y)
Multiplicative data looks like a left sideways triangle: y* = natural_log(y)
Fit model with y* as dependent variable

Tests for heteroscedasticity available


### Assumption #3: e (error term) has a normal distribution
A "Robust" model.  We are saying that when the assumptions are slightly violoated it doesnt effect the model that much.
Violated if non normal errors or outliers

FOr this assumption its almost always better to just look at a graph.
Before making a decision here, you need to check for outliers

Log transformation is stronger than a square root.

Outliers: Extremely large/small residuals, due to:
a. invalid measurement (e.g. coding error)
b. skewness (nonnormality)
c. (measurement from another population
d. misspecified model
e. change/uexplainable causes
