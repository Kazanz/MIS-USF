Parallel Databases: Computation and Communication

Why parallel: Thorughput and response time
Opportunities
Performance metrics
- Speedup
- Scaleup
Limitations
Parallel programming (Linda & JavaSpaces)
Parallel Architecture
Parallel Database Execution (PX)

Aims at improving performance by decomposing the tasks into smaller tasks and execute them in parallel.
Can be single-core, multi-pricessor or collections of computeres in a cluster, grid, or cloud
- Additional computing resources can be simply applied to existing algorithms therby speeding up computation.
- New parallel algorithms can be developed that better use parallel computing resources to solve problems.

Performance
- Throughput: how many transactions are being servered.  (queries per second)
- Response time: time for single transaction.

Availability
- Processor and storage redundncy can lead to a high availability, but highly parallel systems are more likely to suffer component failures

Extensibility
- Maintain performance goals while allowing for growth.

The Cray-1 supercomputer. Typically used for vector or matrix computation, scientific computations.

Computer system: Procedural programming language -> Compiler or Interpretor -> \
                                                                                exeuteable software program or algorithm
Database system: Declarative Language            -> Optimizer               -> /

Speedup: a fixed-sized problem executing on a small system is given to a system which is N times larger

speedup = small_system_elapsed_time / big_system_elapsed_time

Linear Speedup
- A linear performance increase given a linear computing power increase.

Sublinear Speedup
- The problem is that sublinear speedup is inevitable due to overhead costs (no free lunch - NFL).

Scaleup: increase both the size of both the problem and the system; N-Times larger system used to perform N-times larger job

scaleup = small_sustem_small_problem_time / big_sys_big_prob_time

Linear scaleup:
- A sustained level of performance given a linear increase in both computing power and database.
Sublinear Scaleup:
- Still no free lunch.

Transaction Scaleup
- consists of N-times as many clients, submiting N-times as many requests agains an N time larger

IMPORTANT: CHECK NOTES WE MNISSED SOME.

Limitations:

Startup/shutdown costs
- a parallel computaion requires some startup work to plan the parallel execution and divide up the data
- the commputation must also be shutdown as the different intermediate results are merged to form the final solution.

Corrdination / Interferene costs
- Shared resources must be coordinated, especially when there are dependencies or race conditions in the computation

Skew (Unbalances Tasks)
- Data skew
- Execution (Query) Skew

Coordination and interference Costs
- When a new processor is added to most computer designs it slows every other computer down just a little bit (interference).  If the interference is 1% then the maximum speedup is 37 and a 1000 processor system has 4% of the effective power of a single -processor system.

Coordination costs can be minimized (but not removed) through well-designed parallel algorithms and by choosing a suitable level of granularity for the computation, the coarser the granularity, the less coordination required.


Architecture

- Shared memory (most common)
Shared memoery offer high perforamcne and simplicity.
This is a widely available and afforadle architecture that is a natural fit with most relational database systems
Downside is the limited scalability

- Shared disk
appraoaches jettison shared memory
There is a need now for a more general (and complex) communicatoin mechanizm
There is also a new "cache" coherency challenge across multiple memories

- Shared nothing (Most efficient)
Look just like computer networks
There is obviously a need for a general comm mechanism but the whole is hugely scalable.
See NoSQL databases.
One set of data is not dependent on other sets of data.

- Hierarchical
Typically combines memeory modules with a higher level interconnection network.
Complex, but on a "board" computatoins are fast and there is also across network sharing.

Interconnection Networks
- Interconnection networks are used to handle inter-process communication (IPC)
- Options include : bus, mesh, tree and hypercube
- The shred disk architectures often included a high speed bus as an interconnect.

Interconnection networks:

Hypercubes: each corner of cube is a individual computer or chip.
Parallel computing market crashed in 90s.

Relational queries are ideally suited to parallel exeution they consist of uniform operations applied to uniform streams of data.  Each operator produces a new relation so operatiors can be composed into highly parallel dataflow graphs.
Relational set-oriented operations are a natural fit.
Sophisticated optimaizers can generate the necessary parallele executions plans
Symmetric multiprocessor (SMP) machines provide a cost effecgtive platform for database parallelism.

Inter-query parallelism
- a collection of transactions exeute in parallel thereby improving throughput
- This is a the low hanging fruit of database parallelism
Imporves throughput
Transactions execute n parallel
throughput increases
easiest type of parallelism to support
suitable for shared memory architecture where a sequential database system can buse concurrenct to achive parallelism
more difficult for shared disk or shared nothing architecture
- whenever multiple memory areas are used there is a "cache coherency" problem related to memeory synchronization.

Cache Coherency
For shared-sk and shared nothing machines, locking and logging must be coordinated through a communication mechanism
This extra communicatoin adds tot eh overhead costs incurred as part of any parallel execution plan

Cache coherency protocol (CCP)
- before read/write obtain shared/exclusive lock.
retrieve block from disk

Intra-query parallelism
- The compononets of a single transaction execute in parallel,
thereby improving "response time".
- Intra-operation parallelism
- Inter-operation parallelism
  - independent parallelism
  - pipelined parallelism
improves response time.
parallel sorts
parallel joins
Best opportunities for parallelism stem from dividing up the data (within an operation) or by divvying up the operations.

Two interesting perspectives on parallelism
- Lots of data can be easiliy divided up for specific parallelized oeprations.
- Complex queries can be decomposed into independently executable pieces.

Classic book on project management:  Mythical Man-Month
