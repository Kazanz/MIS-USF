# Class 2

## Recap

**Relay an example with a concept.**

There is an infinite number of combinations so the more variables we have the higher chance we have of finding a correlation.
The problem gets worse the more columns you add.

You dont want to say you have a good story.  You want to say you have a good story that is tested.
If you focus on evaluation you will be a good data scientist, (even if you treat the algorithms like a black box)

Medical Diagnosis: Its very common for people to go to multiple doctors so that you get the information right.

If errors of the models match perfectly or close, there is no real benifit of combining them.

## Data minig process

Credit card fraud costs the industry billinos of dollars each year and pattern discovery tools and machine learning models
such as nueral netwlrs areroutinely used to analyze fraud databases to identify triggers.


"A self-service transactions at a gas station followed by an expensive purchage" that is indicative of fraud.
- And example pattern learned from credit card fraud data
- Pattern is then used in real-time to flag transactions that might be fraudulent.  For instance,
if you fill your car with gas with a credit card and then make an expensive purchase then the merchant may
be instructed (by the point of seller system) to check the users ID.


Thief data changes over time (so going back in time it could hurt you).
4 years for fraud is a rather long time.
Consumer data is fairly static over the years.

### Analytics in "real" life

Large number of eHarmony, Match etc are data scientists.

Area: People Analytics (Could go with my research on digitally administered CBT)
Justin Zormelo
World cup soccer in Brasil.  Germany scored 7 goals in the first half. The german team spent alot of time
analyzing Brazilian players and finding little weaknesses in that game.
Crowdsourcing video tracking

**2017 MIT sports analytics conference**

Stock trading is a big example of where algorithms are doing realtime trading.
Call / Put ratio gives you sentiment of a stock.  If sentiment rises.

Black box trading, algo trading
Flash crash of 2:45

1. You always start with secondary data.  (you almost always get data that was collected from something else)
Primary data is collecting data specifically to solve a problem.

When working with secondary data there is a few problems (some you may not be able to do anything about).
The gold standard is experimentation.

**Book to read: Statistics for Experimenters by George EP box, WIllima G hunter.**
Natural experiments

Finding casuality is hard.  Particularly with secondary data.
Also, the data could have a strong bias which must be stated.

The ideal data is a good representation of real data.

2. Create a feature vector

### Patterns vs Models

- Querying large databases
- Learning patterns from data
-- e.g. users who search for aitline tickets are likely to browse the travel section.
- Building models from data
-- USF applicant details -> Projected GPA

When building models - start with the end in mind

A pattern is howing you something narrow about your domain.
A model is a forumla that tells you something about the entire domain.

Data mining == Learning "structure" that is hidden in the data
- "Reverse engineering"
- "Structure" could be patterns or models

A pattern typically explains fsome local structure that might exist in the data

### Supervised Learning
Objective is to learn how specific features of a given object can be mapped to a target feature
Supervised learning has the right answer to start with.

### Unsupervised Learning
Problems for which the "right answer" is not known or given to you, but the goal is to use the data to make sueful discoveries.
LEarning associations or clusters in the data are two common examples.

### Representation Evaluation and Search

A simple framework for understanding DM
- DM techinueqs first have to make a explicity choice or assupmtion regarding what forms a pattern can take
    - specifically a representational form has to be choses

Given a representation, what makes a specific pattern or model in this representation "good"
- Hence you need an evaluation criterion

Given a representation and a method of evaluation, search is the process of learning patterns ormodels in that representation that meet the specified evaluation criteria.
Search is essentially the algo


## The  Data Mining Process:
The most important part is the testing

- Training set: used to build a set of DM models
- Validation set: used to choose best DM models
- Test set: used to determine how the models perform

Often data mining algorithms will automatically partition the data you provide into two groups
- First is training + validation
- Second is testing where it will give you scores for that set

Key distinction: the models are NEVER permitted to "learn" form the test set.

When using temporal data, you want the past data as the train set and the future data as a test set. 
Does not usually make sense to use the current to predict the past.

### Methodology

1. Business Problem -> DM problem.
2. Extract the variables you need.
3. Create the variables you need.
4. Get to know the data by descriptive statistics and exploration (visualizations etc) Usually visualizations points to flaws in the data like age > 200
5. Fix data probelms (variables with too many values (zip code, product name, etc) may need to be grouped, since the data may be sparse at the level of each value
Identify missing values. Use "imputation" techniques to fill these in (I will just ignore it by deleting the record, put in fake information into the holes, like using the average).
Also, Identify outliers and deal with them.
6. Create a modeling data set.
7. Use feature selection. These are algorithms that will select a subset of the large number of variables input into a mode.
Generally considered useful when dealing with extremely high dimentional data.
Feature selection is not always more accurate.
8. Build and assess models
9. Test
10. And reiterate.
