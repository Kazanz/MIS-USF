# Nueral Networks

## Feed forward nueral networks

Layer 1 in the input layer
Layer 2 in the hidden layer
Final Layer predicts the outputs

Every link has a weight

It will ultimately be one linear equation. (Or a Linear Regression model)

A nueral network is only interesting when it uses a "squahing" or "transfer" function in each hidden node.
### Types of transfer functions
Logistic/Sigmoid (creates a S shaped curve)
Linear
Exponential

Back propogation is used to train nueral networks quickly.  (Throwing the ball to the child 1000 times)

In the last 5 years training multi layer nueral networks (deep learning) have been research heavily.
One specific problem driving deep learning is image classification.

Any time multiple sequential transformations deep learning is a useful technique.

Surface-fitters: Good thing it can learn all the non linear shapes in your data.  Bad thing it can overfit.

The more you increase nodes the higher your chance of overfitting and the higher the training time.  But you do lower your potential error.

## How to learn a network from data

- The structure (number of nodes in the hidden layer) and 
  - The number of hidden nodes is trial and error usually
- The actual weights of each connection
  - Backpropogation is a good choice.
    

### Back propogation
- start with a random set of weights
- repeat for fixed number of generations (epoch)
- repeat through the training data set (each generation)
- select next data point from the training data
- use the current nn to make a prediction
- calculate the error made by the NN
- Modify each weight by a small smount to reduce the error (uses the partial derivative  of partial_deriv-ei/partial_deriv-wi
  - learning rate and momentum paramerters come in here
  - calculate the overall error in the training set
- Reaches the end of Generation

Gradient descent technique.  Gently descend the error until you reach the lowest point (the local optimum).
The Global Optimum is the best local optimum.
if you dont get zero you cannot claim that your got the best global optimum.

What poeple typically do, is set a threshold that they want and run the network training until they get it. or they set a number of training tests (1000 lets say) and go until they get the best one.

Learning Rate: 

Hueristic: Its a rule of thumb method, its a good solution, works a good amount of time.  You compare that to the optimal solution.

